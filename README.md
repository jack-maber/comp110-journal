# comp110-journal
## Research Paper 1 - "When does a physical system compute?"
In the first research paper, the overlying theme is what actually defines a computer, a computation, and the physical and theoretical link between the two. As everyone thinks of of a "computer" being the tower under the desk or the laptop jammed into their backpack, this paper tries to break open what actually defines a computer, because as stated in the abstract, "There has been, however,
no consensus on how to tell if a given physical system is acting as a computer or not" and even that "every physical event is a
computation", which even to me seems quite an outlandish claim as even I know that by definition a "physical system" can only really  be defined as a computer if it has the capacity writing and decoding some kind of data so that it can actually carry out a function as a result of that input or output. 

However this paper focuses more on the relation between the aforementioned "physical system" and the "abstract computation/theory" and how we have to have a real world example to explain an abstract theory, especially when explaining physics, which is what the bulk of computational theory is based around at this low level, and this makes perfect sense in any context really as anything without a physical representation can only be explained with an example that exists in the real world, an example given in the paper is "atom is represented as a wave function", everyone should know what an atom is, and where it exists in real world terms, but not everyone knows what a wave function is and where it stands in the real world, and thus this "abstract" theory can be explained a physical representation, which is very important when it comes to understanding the question that the paper is aiming to answer. When it comes down to it, a physical system computes when it has the ability to take a piece of "abstract" data and encode it, and the encoding is not governed purely by the physics of the system, as different systems would be effected differently, also how the computer's data is represented, and stated in the paper, normal semi-conductor based computers rely on the old way of representing "1" with high voltage and "0", and as seen in the highly complex diagrams in the paper, it is of the upmost importance that the relation between the "Physical system" and the way that the "abstract theory" makes it as accurate as possible for the first computation of the data to occur, as otherwise none of the data that follows will make any sense. 

This paper is similar in many ways to paper 3 as it is describing as they are both describing physical theories that can be in someway transferred to computing, with this paper providing a framework to describe when a system is computing and the link between "abstract" and "physical", while Paper 3 proposes a way of measuring the distance objects in 3D space, which can then be transferred into computer programs, as such they both link how real life theories can be transferred and used in computational instances and can be used to great effect. 

## Research Paper 2 - "Experimental Investigations of the Utility of Detailed Flowcharts in Programming "
The second paper focuses on the use of Flowcharts and their effectiveness to aid programmers or academics to understand how a program functions. The creators of this paper set out to investigate this by composing a number of separate experiments to see fit they help programmers in different situations that are commonly encountered in computing, such as comprehension and composition of programs, all very important to becoming a competent programmer, and so any help that Flowcharts could provide would be very useful, which is why the creators set out to create a definitive answer.

I was quickly able to infer from the paper that the creators had carried out a lot of research prior to conducting their experiments, even finding that there had been a proposal for a US National standard in 1963, which I found very surprising as I thought that standardising a creative system such as flowcharts would be quite difficult, but then again it was only proposed, previous papers sourced in the paper, such as Weinburg stated that utilising flowcharts to teach people as they only make sense to the original creator, but again other research inferred other results, but there was no concrete evidence to support any of these claims, which was weird as flowcharts to map programs had been around since the infancy of computers, which brings up a similarity between two of the papers, as paper 4 also calls back to a commonly used technique in computing at the time, the go to statement, and they both provide evidence against these two widely used techniques and disprove their usefulness inn certain areas, as the experiments carried out later in the program prove, only during the creation of programs do flowcharts actually help, whereas in all the other tests it made no difference, or in the case of comprehension it made it easier to have the actual program, which made perfect sense to me as the flowchart becomes redundant, so nothing really outstanding from this research paper, but a good amount of insight into providing concrete evidence,and by covering all of the basic tasks that a programmer will undertake, they've covered all bases, which is a good way of creating solid evidence. 

## Research Paper 3 - "A Fast Procedure for Computing the Distance Between Complex Objects in Three-Dimensional Space"
The third paper presents an algorithm for calculating the distance between objects in 3D Space, namely the "Euclidean distance", which is the shortest line between that could join the two objects, and although there were already methods similar to this at the time (1988) they weren't very "efficient for practical problems where M is large, but not exceedingly large." Which I can only assume is that amount of vertices featured on an object, as this was mentioned earlier, however this kind of algorithm would not be good in any application that featured a high number of objects in said 3D Space, such as video games, while the method applied here aims to much more efficient due to the fact that it works off of the distances between the actual faces of the objects, and thus can work with a "complex family of shape models", as you'd again find in video games and other things such as CAD/CAM software, along with the fact that it is dynamic and can be used when "Objects are subject to changes in position and orientation", again making it useful in a video games context because of the large amount of movement that occurs during gameplay. I can only infer that this method must have been quite revolutionary at the time as stated before only algorithms that worked in 2D and poorly in 3D were available, and as it has so many different applications such as CAD/CAM design, robotics and other computer graphics related fields.

## Research Paper 4 - "Letters to the editor - Go To Statement Considered Harmful"
The 4th and final paper I will be covering is focusing around the use of the "go to" statement in coding, and although the statement has become largely redundant now due to the fact that there are more efficient alternatives and the fact that most languages have become "structured", the example is still relevant for computing overall. The writer of this article, "Edsger W. Dijkstra" argues that the use of the "go to" statement, although still widely used at the time was counter intuitive and affected the readability of the code and the creator's ability to trace back errors, as the "go to" statement was used simply to jump from label to label in the code, over a long period of development, with multiple iterations being created, the users ability to trace where the "go to" statements would become increasingly difficult and make maintaining the code increasingly difficult, which when working with low level code I can imagine is very important due to it's inherently complex nature, and if we go back to the date that this paper was published (1968) I can only infer that this is the majority of programmers would have been working, and to add to this I can only infer that there would have been no dedicated debugging tools available at the time, which may have helped, but again irrelevant because we are focusing on that particular period of time. The links with other papers I can make here is that they are all trying to improve the computing field, with the first paper trying to get the reader to better understand computational thinking, the second looking at if flowcharts are any use to programmers and if they can improve workflow, the third creating a more efficient algorithm, and the 4th debates the "go to" statements use in code and it's counter intuitiveness.  
